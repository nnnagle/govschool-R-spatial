---
title: "01_plot_counties.Rmd"
output: html_document
---

# Planning my attack
When I plan my analyses, I usually do it in in this order: First, I think about what my *research question* is. Second, I think about what I need to *measure*, third, I think about how I will *communicate* that to my audience. I can do all of this before I have data, and doing this helps me to stay on track. Everything I do must help me to reach this goal.

For this tutorial: First, I want to know what the relationship is between race and the incidence of COVID-19. Second, I think that I will need to measure the fraction of each county with COVID-19, and the fraction of each county  that is Black or African-American. Third, I think that I want to communicate this relationship with maps of percent black and percent with COVID, and perhaps with a correlation coefficient between the two. In this tutorial, though, I'll just do the mapping.

How will I do this? I'll need the following data:

Percent Black or African-American for each county.  Where will I get this? The Census Bureau. I might find Percent Black, but I might also only find the Population that is Black or African American, in which case I'll also need the Total Population so that I calculate the Percent.

I'll also need the percent of people in each county that have had COVID-19. Where will I get this? If I am working with a single state, I might go to the State Department of Health, and see if they have data to download. If I am working with the entire nation, I might go to someplace like the Johns Hopkins University dataset that has done a lot of work to collect data from all 50 states and to put them in one file. Again, I might not find the percent, so I might need to get the number of total cases and the Total Population.

I'll also need some way to map the data.  I need data with the name and boundary of each county.

The data have already been downloaded and cleaned and put in the data folder. If you finish with the mapping tutorial and are looking for something a little harder, go ahead and look at file 00_get_data.Rmd where I show how I collected and cleaned these data. You could, for example, follow the tutorial and get the most recent version of the COVID data; the file here is already a week old, which is pretty ancient as far as COVID goes.

# Data
The data are in three files. First, there are files with the name 'cb_2018_us_county-20'. These files go together: even though there are many files, they all belong together and they are called a "shapefile". A shapefile is the most common format for sharing geographic data. This shapefile just contains information about the boundaries of every county in the US.

The second file is called `tn_county_acs.csv` and contains basic demographic characteristics from the census bureau for each county in Tennessee.

The third file is called `tn_covid.csv` and contains data from the Tennessee Department of Health about cases of COVID-19 on each day in each county.

The three files will need to be joined into one file. Fortunately, all three files contain a column called "GEOID" that has a 5-digit number that is unique to each county. This didn't happen magically, it took a little work to get the GEOID column into the COVID data, but that is described in the data tutorial.

We'll need a couple R packages. I like to use `tidyverse` for filtering and merging data.  The `ggplot2` package is the easiest for quickly producing high quality visualizations. And the `sf` package is what you need to read shapefiles.

```{r}
library(tidyverse)
library(ggplot2)
library(sf)
```

First, read in the shapefile:

```{r}
counties <- st_read('data/cb_2018_us_county_20m.shp')
counties
```

You should see that `counties` is a table, just like normal data. There's a bunch of columns we don't care about, but GEOID and NAME are important. Also, there is the column called `geometry`.  You don't need to worry about that yourself, but R needs that to produce a map.

The first column is a two digit "State FIPS code".  This is every county in the US.  I don't need every county, just those Tennessee. The FIPS code for Tennessee is '47'. So, using the `filter` function in the `tidyverse` package, we can filter our data down to just Tennessee:

```{r}
counties <- filter(counties, STATEFP=='47')
```


Next we can read in the covid data and the demographic data that are stored in comma separated files. I use the `read_csv` function in the `tidyverse` package.

```{r}
tn_covid <- read_csv('data/tn_covid.csv')
tn_county_acs <- read_csv('data/tn_county_acs.csv')
```

Look at the `tn_covid` data:
```{r}
tn_covid
```

You'll see that it has good things, like the number of cases and the number of hospitalizations.  It's also got data for every day. That's helpful for looking at time-series trends, but right now I'm looking at spatial trends, not time trends -so let's just filter down to the most recent day.

```{r}
tn_covid <- filter(tn_covid, DATE==max(DATE)) # I used max() to get the most recent date.
```

We have our three dataset: the boundaries, the COVID data, and the demographic data, so we ar ready to join them together. Fortunately, they all have the column `GEOID` to use for joining them.

```{r, error=TRUE}
counties <- 
  left_join(x = counties,
            y = tn_county_acs,
            by = 'GEOID')
```

you get an error telling you that one dataset has a column called GEOID that is a factor ('categorical') variable, and the other has a column called GEOID that is numeric.  It's true, a code called something like "47001" could be numeric, but we should really treat it the same as a name.

I will use the `left_join` function to gather in variables from the Census to the shapefile, and do the same for the COVID data. As I do so, I will also convert the GEOID column to text/character so they are the same.

```{r}
counties <- 
  left_join(x = counties %>% mutate(GEOID = as.character(GEOID)),
            y = tn_county_acs %>% mutate(GEOID=as.character(GEOID)),
            by = 'GEOID')
counties <- 
  left_join(x=counties,
            y = tn_covid %>% mutate(GEOID = as.character(GEOID)),
            by = 'GEOID')
```

As one more thing, I'll convert the COVID rate from number of cases to number of cases per 10,000 people.

```{r}
counties <- mutate(counties,
                   COVID_RATE = TOTAL_CASES / population * 10000)
```


```{r}
ggplot(data=counties) +
  geom_sf(mapping = aes(fill=COVID_RATE))
```

We only see three counties. A common challenge in mapping is that a few outliers mask all the other data. In this case, these three counties just happend to be small counties, that have large prisons, where there has been an outbreak. We should look at what values are typical:

```{r}
quantile(counties$COVID_RATE)
```

Cartographers have done a lot of research into wehat makes maps easier to use. Much of this research points toward mapping using discrete colors instead of continuous colors. I use the `cut` function to turn a continuous variable to categorical.

```{r}
counties$covid_rate_cat <- cut(counties$COVID_RATE,
                               breaks=c(0,5,10,20,50,2000),
                               ordered_result = TRUE)
```

And make the map with this new categorical variable:
```{r}
ggplot(data=counties) +
  geom_sf(mapping = aes(fill=covid_rate_cat))
```

## Clean up the axes

Axes are important in most data visualizations. In this viz, the color legend is necessary. But are the x and y axes important? Does knowing the longitude and latitude help to understand the data? No, they just get in the way.  So let's remove them. For the record, a lot of things that people associate with maps, like North arrows and scale bars, aren't always imortant. If someone is looking at a map of Tennessee and the don't know which way North is, they are probably going to struggle understanding it no matter what you do.

The following code gets rid of the axes:
```{r}
ggplot(data=counties) +
  geom_sf(mapping = aes(fill=covid_rate_cat)) + 
  coord_sf(datum = NA)
```

If you want to get rid of the background as well as the axes, you can do this:

```{r}
ggplot(data=counties) +
  geom_sf(mapping = aes(fill=covid_rate_cat)) + 
  theme_void()
```

You may wonder who anyone can ever remember all these recipes. The truth is that I don't.  I usually google them.

## Changing the legend label

It's always good to change the legend and add a title. The legend goes with the scale of the color fill, so I use the `scale_fill_ordinal` function:

```{r}
ggplot(data=counties) +
  geom_sf(mapping = aes(fill=covid_rate_cat)) + 
  theme_void() +
  scale_fill_ordinal('Rate (Cases per 10,000 persons)') + 
  ggtitle('Total Covid Cases (up to June 6, 2020)')
```

## Changing the color scale
The basic colors that are used for mapping in ggplot aren't great. There's actually a science behind what colors to use. You want each color to be easily distinguished from the others, and you want to make sure that colorblind people can use your map too. The best resource for different colors is https://colorbrewer2.org/. Fortunately, these are available in R by changing the `scale_fill_`:


```{r}
ggplot(data=counties) +
  geom_sf(mapping = aes(fill=covid_rate_cat)) + 
  theme_void() +
  scale_fill_brewer('Rate (Cases per 10,000 persons)',
                    palette='YlOrRd') +
  ggtitle('Total Covid Cases (up to June 6, 2020)')
```
You can try other palettes too. Type `help(scale_fill_brewer)` for a list of palettes. Most choices have obvious names, like 'Reds', 'Blues', 'YlGnBu' and 'OrRd'.

# Map the Percent Black of African American
Here's a map of the Percent that is Black or African American.  I have just followed the same pattern as before.

```{r}
counties <- mutate(counties,
                   pct_black_cat = cut(pct_black, 
                                       breaks = c(0, 2, 5, 10, 20, 40, 100)))
ggplot(data=counties) +
  geom_sf(mapping = aes(fill=pct_black_cat)) + 
  theme_void() +
  scale_fill_brewer('Percent',
                    palette='YlOrRd') + 
  ggtitle('Percent Black or African-American')
```

Finally, youl will want to save your plots so that you can insert the into a word document. Use the `ggsave` function, like so:

```{r}
ggsave(filename = 'Black_Tennessee.png', height = 4, width=6)
```

This will save the last plot to a file. I find that a 4 in by 6 in plot is usually about the right size for a word document, with a little bit of resizing inside word.  Common file formats are '.png' and '.pdf'